# Deep Deterministic Policy Gradient (DDPG) implemetation Report

### Introduction
This repository is an implementation of the Deep Deterministic Policy Gradient (DDPG) algorithm for the Reacher Environment developed by Unity3D and accessed through the UnityEnvironment library. It is an extension of the code sample provided by the Udacity Deep RL teaching crew (for more information visit their [website](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)). The environment is presented as a vector; thus, we did not use Convolutional Neural Networks (CNN) \[[1](http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf)\] in the implementation.

The DQN algorithm and its derivatives were performing well in certain situations, but they were suffering from many shortcomings. The most important was their inability to tackle continuous action environments. The main reason for this is the curse of dimensionality present in these environments, which makes them slow even after discretization. Thus, the Deep Deterministic Policy Gradient (DDPG) was proposed in 2016 \[[2](https://arxiv.org/abs/1509.02971)\], which was an enhanced version of the original DPG paper \[[3](http://proceedings.mlr.press/v32/silver14.pdf)\]. DDPG is inherently model-free and actor-critic \[[2](https://arxiv.org/abs/1509.02971)\].

DDPG borrowed the main algorithm from the DPG paper, but in architecture and learning mechanism, it is following the footsteps of the DQN algorithm \[[4](https://arxiv.org/abs/1312.5602)\], namely using experience replay and the target network. The actor-critic nature of DDPG requires the initialization of two interacting neural networks. In each iteration, the actor-network generates an action according to the current policy and Ornstein-Uhlenbeck random process as its exploration noise. After performing the chosen action, observing the new state and receiving the reward, the critic network tries to minimize the loss function, which also influences the actor policy. The final step is to update the target network using the <img src="https://render.githubusercontent.com/render/math?math=\tau"> hyperparameter, which controls the amount of update. Instead of the delayed update method in DQN, DDPG instantly updates the target function using <img src="https://render.githubusercontent.com/render/math?math=\tau">. The DDPG pseudocode is available in algorithm Figure below.

For consistancy, DDPG follows DQN's architecture which had five-layer neural network architecture for the pixel-based training. The input to the network is the environment state, and its output is the set of Q-values for the critic and the action policy for the actor. When a vectorized environment is used, the CNN part can be cautiously removed, utilizing the provided vectors as features generated by the CNN. DDPG success can be attributed to two main innovative techniques:
1. Actor-Critic Paradigm
2. Deterministic Policy
3. Efficient way of using Neural Nets (and Target Networks)
4. Experience Replay

### Method
DQN, as represented in Figure 1, executes a typical reinforcement learning algorithm. It gathers a repository of experiences or transitions while exploring the environment. This dataset is collected by a behavior policy which is being updated more regularly. The target policy, which determines the final policy of the agent, is updated on a slower rate.

<center><img src="https://raw.githubusercontent.com/FredAmouzgar/DDPG_PyTorch/master/pics/ddpg.png" width="800" height="400">
<br><font size=2>Figure 1: The DDPG <a href="https://arxiv.org/abs/1312.5602">[2]</a></font></center>

### The Reacher Environment
The example uses a modified version of the Unity ML-Agents Reacher Example Environment. The environment includes In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible. The environment uses multiple unity agents to increase training time.

<img src="https://github.com/FredAmouzgar/DDPG_PyTorch/raw/master/images/Reacher.png" width="400" height="200">

The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.


### Experiments
This repository consists of these files:

*These files are saved under the "src" directory.*
1. <ins> model.py </ins>: This module provides the underlying neural network for our agent. When we train our agent, this neural network is going to be updated by backpropagation.
2. <ins>replay_buffer.py</ins>: This module implements the "memory" of our agent, also known as the Experience Replay.
3. <ins>agent.py</ins>: This is the body of our agent. It implements the way the agent acts using an actor-critic paradigm, and learn an optimal policy.
4. <ins>train.py</ins>: This module has the train function which takes the agent, the environment, number of training episodes and the required hyper-parameters and trains the agent accordingly.
*These files are saved under the "src" directory.*

To test the code, after cloning the project, open the `Reacher_Continuous_Control.ipynb` notebook. It has all the necessary steps to install and load the packages, and train and test the agent. It also automatically detects the operating system, and loads the corresponding environment. There is an already trained agent stored in `checkpoint-actor.pth` and `checkpoint-critic.pth`, by running the last part of the notebook, this can be directly tested.

Figure 2 is depicted a reward plot acquired by the agent while learning. It surpasses +33 after around 120 episodes.

<center><img src="https://github.com/FredAmouzgar/DDPG_PyTorch/raw/master/images/DDPG_reward_plot.png" width="400" height="200"><br><font size=2>Figure 2: The average reward during training</font></center>

Figure 3 shows one episode after training.

<center><img src="https://github.com/FredAmouzgar/DDPG_PyTorch/raw/master/images/Reacher.gif" width="400" height="200"><br><font size=2>Figure 3: A Trained Agent</font></center>

### Hyperparameters
Reacher Environment:
- State size: 33
- Action size: 4


DQN Agent:
- Replay memory buffer size: <img src="https://render.githubusercontent.com/render/math?math=10 ^ 5">
- Batch size: 64
- <img src="https://render.githubusercontent.com/render/math?math=\gamma">: 0.99
- <img src="https://render.githubusercontent.com/render/math?math=\tau"> for target update: <img src="https://render.githubusercontent.com/render/math?math=10^{-3}">
- <img src="https://render.githubusercontent.com/render/math?math=\alpha"> or learning rate: <img src="https://render.githubusercontent.com/render/math?math=5\times 10 ^ {-4}">

Neural Networks:
- Input Layer: 37 (state size)
- Layer 1: 64 units
- Layer 2: 64 units
- Output Layer: 4 (action size)
- Activation function: ReLU, linear (output layer)

Training Parameters:
- Episodes: 1200
- Steps in every episode: 1000
- First <img src="https://render.githubusercontent.com/render/math?math=\epsilon"> value: 1.0
- Final <img src="https://render.githubusercontent.com/render/math?math=\epsilon"> value: 0.01
- <img src="https://render.githubusercontent.com/render/math?math=\epsilon"> decay: 0.995

### Future Work:
1. Implementing TD3
2. Implementing D4PG
3. Trying other hyperparameters